{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Clean Data\n",
    "\n",
    "2019 - 01 - 21 updated 2019-04-30\n",
    "Notebook to clean final osm cities and towns\n",
    "1. merge files\n",
    "2. drop duplicates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import json\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tana-crunch/cascade/projects/Pop-ERL/notebooks/jupyter/ERL19'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "\n",
    "ERLv2_data = '../../../temp_data/ERL19v2/'\n",
    "temp_data = '../../../temp_data/ERL19/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge OSM Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points (file):\n",
    "    \"\"\" This function loads a csv \n",
    "    of points and turns it into shapely points\"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # creating a geometry column \n",
    "    geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "\n",
    "    # Coordinate reference system : WGS84\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "\n",
    "    # Creating a Geographic data frame \n",
    "    point_gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "    \n",
    "    return point_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Files\n",
    "town = load_points(v2_data+'20190114_osm_africa_towns.csv')\n",
    "city = load_points(v2_data+'20190114_osm_africa_cities.csv')\n",
    "\n",
    "townSS = load_points(v2_data+'20190221_osm_S_Sudan_towns.csv')\n",
    "citySS = load_points(v2_data+'20190221_osm_S_Sudan_cities.csv')\n",
    "\n",
    "townDRC = load_points(v2_data+'20190430_osm_DRC_towns.csv')\n",
    "cityDRC = load_points(v2_data+'20190430_osm_DRC_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with osm type\n",
    "\n",
    "town['osm_type'] = 'town'\n",
    "townDRC['osm_type'] = 'town'\n",
    "townSS['osm_type'] = 'town'\n",
    "\n",
    "city['osm_type'] = 'city'\n",
    "cityDRC['osm_type'] = 'city'\n",
    "citySS['osm_type'] = 'city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityDRC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "\n",
    "towns = pd.concat([town, townDRC, townSS, cityDRC, citySS], sort = False) # col name of cityDRC and citySS is town, not city\n",
    "\n",
    "towns.rename(columns={'town':'osm_name'}, inplace=True)\n",
    "city.rename(columns={'city':'osm_name'}, inplace=True)\n",
    "\n",
    "out = pd.concat([towns, city], sort = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_file(v2_data+\"20190430_osm_All.shp\", driver = \"ESRI Shapefile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 1500c300 Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "\n",
    "GHS2000 = gpd.read_file(temp_data+'GHS_POP_GPW42000_GLOBE_R2015A_54009_1k_v1_0_Clip_1500c300.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD FIDS if needed\n",
    "\n",
    "FID = list(range(len(GHS2000)))\n",
    "GHS2000['DN'] = FID\n",
    "GHS2000.columns.values[0] = \"FID\"\n",
    "GHS2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GHS2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000_A = GHS2000[0:15000]\n",
    "len(GHS2000_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000_B = GHS2000[15000:30000]\n",
    "len(GHS2000_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000_C = GHS2000[30000:]\n",
    "len(GHS2000_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GHS2000_A)+len(GHS2000_B)+len(GHS2000_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000_A.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000_C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHS2000_A.to_file(temp_data+'GHS_POP_GPW42000_GLOBE_R2015A_54009_1k_v1_0_Clip_1500c300_A.shp')\n",
    "GHS2000_B.to_file(temp_data+'GHS_POP_GPW42000_GLOBE_R2015A_54009_1k_v1_0_Clip_1500c300_B.shp')\n",
    "GHS2000_C.to_file(temp_data+'GHS_POP_GPW42000_GLOBE_R2015A_54009_1k_v1_0_Clip_1500c300_C.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge PolyPoints \n",
    "- Merge polypoints outputs & save\n",
    "- Find FIDs that overlap boarders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "poly_A = gpd.read_file(ERLv2_data+'LS15_w001001_Clip_1500c300_A_polypoints.shp')\n",
    "poly_B = gpd.read_file(ERLv2_data+'LS15_w001001_Clip_1500c300_B_polypoints.shp')\n",
    "#poly_C = gpd.read_file(ERLv2_data+'GHS_POP_GPW42015_GLOBE_R2015A_54009_1k_v1_0_Clip_1500c300_C_polypoints.shp')\n",
    "\n",
    "out_data = 'LS2015'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = pd.concat([poly_A, poly_B], ignore_index = True) #poly_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out all Polygons\n",
    "poly.to_file(ERLv2_data+out_data+'_polypoints_ALL.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Country Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files\n",
    "out_data = 'WP2015'\n",
    "\n",
    "#file = out_data+'_polypoints_ALL.shp'\n",
    "file = 'WP2015_1500c300_polypoints.shp'\n",
    "\n",
    "poly_all = gpd.read_file(ERLv2_data+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = poly_all\n",
    "step1['dup'] = poly_all['FID'].astype(str) + poly_all['country'] # Make new col with FID-Country\n",
    "step2 = step1.drop_duplicates('dup', keep='first') # drop country-FID dups, keep first though\n",
    "step3 = step2[step2.duplicated(subset=['FID'], keep=False)] # keep all with duplicated FIDS, drop unique due to no country overlap\n",
    "step4 = step3.drop_duplicates('FID', keep='first') # drop country-FID dups remaining duplicates\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Western Sahara / Morocco and South Sudan / Sudan\n",
    "step4 = step4[step4['country'] != 'Sudan']\n",
    "\n",
    "print(len(step4))\n",
    "\n",
    "step4 = step4[step4['country'] != 'Morocco']\n",
    "print(len(step4))\n",
    "\n",
    "# out = out[out.duplicated(subset=['FID'], keep=False)] # Keep all duplicated FIDS but remove any left overs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4.to_file(ERLv2_data+out_data+'_polypoints_countryoverlap.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIDS with GHS2015_ALL to check\n",
    "\n",
    "testA = poly[poly['FID'] == 27492] # double boarder\n",
    "testB = poly[poly['FID'] == 31036] #lagos\n",
    "testC = poly[poly['FID'] == 187] # random city only in Algeria \n",
    "testD = poly[poly['FID'] == 28] # large city in S africa\n",
    "testE = poly[poly['FID'] == 18] # single city\n",
    "\n",
    "test = pd.concat([testA, testB, testC, testD, testE])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Code to find polygons that overlap boarders\n",
    "\n",
    "# test = poly[poly.duplicated(subset=['FID'], keep=False)] # Keep all dup FIDS\n",
    "# test['dup'] = test['FID'].astype(str) + test['country'] # Make new col with FID-Country\n",
    "# test.head()\n",
    "\n",
    "# out = test.drop_duplicates('dup', keep=False) # Drop all doubles FID-Country\n",
    "# print(len(out))\n",
    "\n",
    "# # out = out.drop_duplicates('FID', keep='first') # Drop all left double FIDs\n",
    "# # print(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake Data\n",
    "\n",
    "fid = [1,2,3,2,2,3]\n",
    "country = ['A','B','C','A','B','A']\n",
    "pop = [10,11,12,11,11,12]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['FID'] = fid\n",
    "df['country'] = country\n",
    "df['pop'] = pop\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1A = poly[poly['FID'] == 31036] #lagos\n",
    "step1B = poly[poly['FID'] == 187] # random city only in Algeria \n",
    "\n",
    "step1 = pd.concat([step1A, step1B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new col with FID-Country\n",
    "\n",
    "step1['dup'] = step1['FID'].astype(str) + step1['country'] # Make new col with FID-Country\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated country-FID, but keep first\n",
    "\n",
    "step2 = step1.drop_duplicates('dup', keep='first')\n",
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all duplicated FIDS, remove singles\n",
    "\n",
    "step3 = step2[step2.duplicated(subset=['FID'], keep=False)] \n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining duplicated FIDS \n",
    "\n",
    "step4 = step3.drop_duplicates('FID', keep='first')\n",
    "step4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
