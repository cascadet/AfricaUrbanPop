{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERL Analysis 2019-02-22\n",
    "\n",
    "Code for fixing the South Sudan and missing GHS data. South Sudan, w/ FIDs duplicates and <5000 dropped\n",
    "FIDS may be duplicated for GHS 2000 & 2015 because of the addition of South Sudan Data\n",
    "\n",
    "-- Cascade Tuholske 2019-02-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "import squarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_drop(gpd_in, col, keep_dup):\n",
    "    \"\"\" \n",
    "    function drops duplicates based on a column from a pd data frame\n",
    "    requires pd df out string, pd df, col name, and which dup to keep\n",
    "    returns new gpd_df\n",
    "    \"\"\"\n",
    "    \n",
    "    gpd_out = gpd.GeoDataFrame()\n",
    "    \n",
    "    print(gpd_in.shape)\n",
    "    \n",
    "    gpd_out = gpd_in.drop_duplicates(col, keep = keep_dup)\n",
    "    \n",
    "    print(gpd_out.shape)\n",
    "\n",
    "    return gpd_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "\n",
    "data_raw = '../../data/raw/'\n",
    "data_temp = '../../temp_data/'\n",
    "data_interim = '../../interim/'\n",
    "data_analysis = '/Users/cascade/Github/NTL/temp_data/ERL_data/Data20190222/'\n",
    "downloads = '/Users/cascade/Downloads/'\n",
    "desktop = '/Users/cascade/Desktop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data: ...20190222.shp files have FIDs removed, S Sudan added, rainfall zones, and regions\n",
    "\n",
    "GHS2000 = gpd.read_file(data_analysis+'GHS_POP_GPW42000_20190222.shp', driver = 'ESRI Shapefile')\n",
    "GHS2000_New = gpd.read_file(data_analysis+'GHS_POP_GPW42000_urbanmerge_PopTot.shp')\n",
    "GHS2000_Sudan = gpd.read_file(data_analysis+'GHS2000_1500c300_S_Sudan_polyoverlapPopTot.shp')\n",
    "\n",
    "GHS2000_old = gpd.read_file(data_analysis+'GHS_POP_GPW42000_final20190122.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [GHS2000, GHS2000_New, GHS2000_Sudan, GHS2000_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sierra_old = GHS2000_old[GHS2000_old.country == 'Egypt']\n",
    "Sierra_old = Sierra_old[Sierra_old.PopTot > 5000]\n",
    "Sierra_old = dup_drop(Sierra_old, 'FID', 'first')\n",
    "\n",
    "Sierra_old.sort_values('PopTot', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Sierra = GHS2000[GHS2000.country == 'Egypt']\n",
    "Sierra.sort_values('PopTot', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_file = 'ERL_data/GHS_POP_GPW42000_urbanmerge'\n",
    "poly_gpd = gpd.read_file(data_temp+poly_file+'.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poly_gpd.crs = {'init': 'epsg:4326'}\n",
    "poly_gpd = poly_gpd.to_crs({'init': 'esri:54009'})\n",
    "poly_gpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gpd.to_file(desktop+'GHS_POP_GPW42000_urbanmerge_54009.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats for Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country = 'Mali'\n",
    "# dataset1 = GHS2000\n",
    "# dataset2 = GHS2015\n",
    "\n",
    "# test = dataset1[dataset1['country'] == country]\n",
    "# test = test[test.PopTot <5*10**6]\n",
    "\n",
    "# print(test.PopTot.count())\n",
    "# print(test.PopTot.median())\n",
    "# print(test.PopTot.mean())\n",
    "# print(test.PopTot.sum())\n",
    "\n",
    "# test = dataset2[dataset2['country'] == country]\n",
    "# test = test[test.PopTot <5*10**6]\n",
    "\n",
    "# print(test.PopTot.count())\n",
    "# print(test.PopTot.median())\n",
    "# print(test.PopTot.mean())\n",
    "# print(test.PopTot.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Data\n",
    "\n",
    "# #GHS - Note, useing the merge files PopTot because I had to re-do the zonal stats \n",
    "# GHS2000 = gpd.read_file(data_analysis+'GHS_POP_GPW42000_urbanmerge_PopTot.shp')\n",
    "# GHS2000_Sudan = gpd.read_file(data_analysis+'GHS2000_1500c300_S_Sudan_polyoverlapPopTot.shp')\n",
    "# GHS2015 = gpd.read_file(data_analysis+'GHS_POP_GPW42015_urbanmerge_PopTot.shp')\n",
    "# GHS2015_Sudan = gpd.read_file(data_analysis+'GHS2015_1500c300_S_Sudan_polyoverlapPopTot.shp')\n",
    "\n",
    "# #WP\n",
    "# WP2000 = gpd.read_file(data_analysis+'AFR_PPP_2000_adj_v2_final20190122.shp')\n",
    "# WP2000_Sudan = gpd.read_file(data_analysis+'AFR_PPP_2015_adj_v2_S_Sudan_1500c300_polyoverlapPopTot.shp')\n",
    "# WP2015 = gpd.read_file(data_analysis+'AFR_PPP_2015_adj_v2_final20190122.shp')\n",
    "# WP2015_Sudan = gpd.read_file(data_analysis+'AFR_PPP_2015_adj_v2_S_Sudan_1500c300_polyoverlapPopTot.shp')\n",
    "\n",
    "\n",
    "# #WPE\n",
    "# WPE2016 = gpd.read_file(data_analysis+'WPE_1KM_2016_final20190122.shp')\n",
    "# WPE2016_Sudan = gpd.read_file(data_analysis+'WPE_1KM_2016_Pop_Clip_S_Sudan_1500c300_polyoverlapPopTot.shp')\n",
    "\n",
    "# #LS \n",
    "# LS2015 = gpd.read_file(data_analysis+'LS15_final20190122.shp')\n",
    "# LS2015_Sudan = gpd.read_file(data_analysis+'LS15_w001001_Clip_S_Sudan_1500c300_polyoverlapPopTot.shp')\n",
    "\n",
    "# print(len(GHS2000))\n",
    "# print(len(GHS2000_Sudan))\n",
    "# print(len(GHS2015))\n",
    "# print(len(GHS2015_Sudan))\n",
    "# print(len(WP2000))\n",
    "# print(len(WP2000_Sudan))\n",
    "# print(len(WP2015))\n",
    "# print(len(WP2015_Sudan))\n",
    "# print(len(WPE2016))\n",
    "# print(len(WPE2016_Sudan))\n",
    "# print(len(LS2015))\n",
    "# print(len(LS2015_Sudan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop FID duplicates ... recall that FIDs for GHS will not be unique for South Sudan Dataset and GHS All \n",
    "\n",
    "# GHS2000 = dup_drop(GHS2000, 'FID', 'first')\n",
    "# GHS2000_Sudan = dup_drop(GHS2000_Sudan, 'FID', 'first')\n",
    "\n",
    "# GHS2015 = dup_drop(GHS2015, 'FID', 'first')\n",
    "# GHS2015_Sudan = dup_drop(GHS2015_Sudan, 'FID', 'first')\n",
    "\n",
    "# WP2000 = dup_drop(WP2000, 'FID', 'first')\n",
    "# WP2000_Sudan = dup_drop(WP2000_Sudan, 'FID', 'first')\n",
    "\n",
    "# WP2015 = dup_drop(WP2015, 'FID', 'first')\n",
    "# WP2015_Sudan = dup_drop(WP2015_Sudan, 'FID', 'first')\n",
    "\n",
    "# WPE2016 = dup_drop(WPE2016, 'FID', 'first')\n",
    "# WPE2016_Sudan = dup_drop(WPE2016_Sudan, 'FID', 'first')\n",
    "\n",
    "# LS2015 = dup_drop(LS2015, 'FID', 'first')\n",
    "# LS2015_Sudan = dup_drop(LS2015_Sudan, 'FID', 'first')\n",
    "\n",
    "# print(len(GHS2000))\n",
    "# print(len(GHS2000_Sudan))\n",
    "# print(len(GHS2015))\n",
    "# print(len(GHS2015_Sudan))\n",
    "# print(len(WP2000))\n",
    "# print(len(WP2000_Sudan))\n",
    "# print(len(WP2015))\n",
    "# print(len(WP2015_Sudan))\n",
    "# print(len(WPE2016))\n",
    "# print(len(WPE2016_Sudan))\n",
    "# print(len(LS2015))\n",
    "# print(len(LS2015_Sudan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in South Sudan ... do each \n",
    "# print(len(GHS2015))\n",
    "# print(len(GHS2015_Sudan))\n",
    "# frames = [GHS2015, GHS2015_Sudan]\n",
    "\n",
    "# GHS2015_merge = pd.concat(frames)\n",
    "# print(len(GHS2015_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop cities with > 5,000 people\n",
    "\n",
    "# print(len(GHS2000_merge))\n",
    "# GHS2000_merge = GHS2000_merge[GHS2000_merge.PopTot >= 5000]\n",
    "# print(len(GHS2000_merge))\n",
    "\n",
    "# print(len(GHS2015_merge))\n",
    "# GHS2015_merge = GHS2015_merge[GHS2015_merge.PopTot >= 5000]\n",
    "# print(len(GHS2015_merge))\n",
    "\n",
    "# print(len(WP2000_merge))\n",
    "# WP2000_merge = WP2000_merge[WP2000_merge.PopTot >= 5000]\n",
    "# print(len(WP2000_merge))\n",
    "\n",
    "# print(len(WP2015_merge))\n",
    "# WP2015_merge = WP2015_merge[WP2015_merge.PopTot >= 5000]\n",
    "# print(len(WP2015_merge))\n",
    "\n",
    "# print(len(LS2015_merge))\n",
    "# LS2015_merge = LS2015_merge[LS2015_merge.PopTot >= 5000]\n",
    "# print(len(LS2015_merge))\n",
    "\n",
    "# print(len(WPE2016_merge))\n",
    "# WPE2016_merge = WPE2016_merge[WPE2016_merge.PopTot >= 5000]\n",
    "# print(len(WPE2016_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset column \n",
    "\n",
    "# GHS2000_merge['dataset'] = 'GHS-Pop 2000'\n",
    "# GHS2015_merge['dataset'] = 'GHS-Pop 2015'\n",
    "# WP2000_merge['dataset'] = 'WorldPop 2000'\n",
    "# WP2015_merge['dataset'] = 'WorldPop 2015'\n",
    "# LS2015_merge['dataset'] = 'LandScan 2015'\n",
    "# WPE2016_merge['dataset'] = 'WPE2016'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [GHS2000_merge, GHS2015_merge, WP2000_merge, WP2015_merge, LS2015_merge, WPE2016_merge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Regions\n",
    "\n",
    "### List of African Countries from the UN in OSM wiki\n",
    "\n",
    "# Northern_Africa = (['Algeria', 'Egypt', 'Libya', 'Morocco', 'Tunisia', 'Western Sahara'], 'Northern_Africa')\n",
    "\n",
    "# Eastern_Africa = ([\n",
    "#     'Burundi',\n",
    "#     'Comoros',\n",
    "#     'Djibouti',\n",
    "#     'Eritrea',\n",
    "#     'Ethiopia',\n",
    "#     'Kenya',\n",
    "#     'Madagascar',\n",
    "#     'Malawi',\n",
    "#     'Mauritius',\n",
    "#     #Mayotte,\n",
    "#     'Mozambique',\n",
    "#     'Réunion',\n",
    "#     'Rwanda',\n",
    "#     'Somalia',\n",
    "#     'Sudan',\n",
    "#     'South Sudan',\n",
    "#     'Uganda',\n",
    "#     'Tanzania',\n",
    "#     'Zambia',\n",
    "#     'Zimbabwe'], 'Eastern_Africa')\n",
    "    \n",
    "# Middle_Africa = ([\n",
    "#     'Angola',\n",
    "#     'Cameroon',\n",
    "#     'Central African Republic',\n",
    "#     'Chad',\n",
    "#     'Congo-Brazzaville',\n",
    "#     'Congo-Kinshasa',\n",
    "#     'Equatorial Guinea',\n",
    "#     'Gabon',\n",
    "#     'Sao Tome and Principe'], 'Middle_Africa')\n",
    "    \n",
    "# Southern_Africa = ([\n",
    "#     'Botswana',\n",
    "#     'Lesotho',\n",
    "#     'Namibia',\n",
    "#     'South Africa',\n",
    "#     'Swaziland'], 'Southern_Africa')\n",
    "    \n",
    "# Western_Africa = ([\n",
    "#     'Benin',\n",
    "#     'Burkina Faso',\n",
    "#     'Cape Verde',\n",
    "#     'Côte d\\'Ivoire',\n",
    "#     'Gambia',\n",
    "#     'Ghana',\n",
    "#     'Guinea',\n",
    "#     'Guinea-Bissau',\n",
    "#     'Liberia',\n",
    "#     'Mali',\n",
    "#     'Mauritania',\n",
    "#     'Niger',\n",
    "#     'Nigeria',\n",
    "#     'Senegal',\n",
    "#     'Sierra Leone',\n",
    "#     'Togo'], 'Western_Africa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions = [Northern_Africa, Western_Africa, Eastern_Africa, Southern_Africa, Middle_Africa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def region_col(gpd_df, regions_tuple, col_name_in, new_col):\n",
    "#     \"\"\"\n",
    "#     Function searchs a col of a data frame and matches it with a list of \n",
    "#     tuples of which [0] countains a list of values to be cross referenced\n",
    "#     & then makes a new col with the tuple [1] ... For example, you have a col with countries\n",
    "#     and you want to make a new col listing the region that country is apart of\n",
    "#     \"\"\"\n",
    "#     arr = []\n",
    "#     for i, row in gpd_df.iterrows():\n",
    "#         for region in regions_tuple:\n",
    "#             for country in region[0]:\n",
    "#                 if row[col_name_in] == country:\n",
    "#                     name = region[1]\n",
    "#                     arr.append(name)\n",
    "#                     break\n",
    "#     gpd_df[new_col] = arr\n",
    "#     return gpd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in datasets:\n",
    "#     dataset = region_col(dataset, regions, 'country', 'region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Rainfall Zone\n",
    "\n",
    "# Group by rainfall zone\n",
    "\n",
    "# arid = (['Temperate / arid', \n",
    "#          'Subtropic - warm / arid', \n",
    "#          'Subtropic - cool / arid', \n",
    "#          'Tropic - warm / arid',\n",
    "#          'Tropic - cool / arid'], 'Arid')\n",
    "\n",
    "# semi_arid = (['Temperate / Semi-arid', \n",
    "#               'Subtropic - warm / semiarid', \n",
    "#               'Subtropic - cool / semiarid',\n",
    "#               'Tropic - warm / semiarid', \n",
    "#               'Tropic - cool / semiarid'], 'Semi-arid')    \n",
    "\n",
    "# sub_humid = (['Temperate / sub-humid', \n",
    "#               'Subtropic - warm / subhumid', \n",
    "#               'Subtropic - cool / subhumid',\n",
    "#               'Tropic - warm / subhumid', \n",
    "#               'Tropic - cool / subhumid'], 'Sub-humid')\n",
    "\n",
    "# humid = (['Temperate / humid', \n",
    "#           'Subtropic - warm / humid', \n",
    "#           'Subtropic - cool / humid', \n",
    "#           'Tropic - warm / humid',\n",
    "#           'Tropic - cool / humid'], 'Humid')\n",
    "\n",
    "# boreal = (['Boreal'], 'Boreal')\n",
    "\n",
    "# na = (['NoClass', '0'], 'NA')\n",
    "\n",
    "# rain_list = [arid, semi_arid, sub_humid, humid, boreal, na]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in datasets:\n",
    "#     dataset = region_col(dataset, rain_list, 'aez_class', 'rain_zone')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
